{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98369101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca043e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is now fully cleaned and preprocessed\n",
      "Remaining number of rows: 2116\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2116 entries, 0 to 2125\n",
      "Data columns (total 35 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   b         2116 non-null   float64\n",
      " 1   e         2116 non-null   float64\n",
      " 2   LB        2116 non-null   float64\n",
      " 3   AC        2116 non-null   float64\n",
      " 4   FM        2116 non-null   float64\n",
      " 5   UC        2116 non-null   float64\n",
      " 6   DL        2116 non-null   float64\n",
      " 7   DS        2116 non-null   float64\n",
      " 8   DP        2116 non-null   float64\n",
      " 9   ASTV      2116 non-null   float64\n",
      " 10  MSTV      2116 non-null   float64\n",
      " 11  ALTV      2116 non-null   float64\n",
      " 12  MLTV      2116 non-null   float64\n",
      " 13  Width     2116 non-null   float64\n",
      " 14  Min       2116 non-null   float64\n",
      " 15  Max       2116 non-null   float64\n",
      " 16  Nmax      2116 non-null   float64\n",
      " 17  Nzeros    2116 non-null   float64\n",
      " 18  Mode      2116 non-null   float64\n",
      " 19  Mean      2116 non-null   float64\n",
      " 20  Median    2116 non-null   float64\n",
      " 21  Variance  2116 non-null   float64\n",
      " 22  Tendency  2116 non-null   float64\n",
      " 23  A         2116 non-null   float64\n",
      " 24  B         2116 non-null   float64\n",
      " 25  C         2116 non-null   float64\n",
      " 26  D         2116 non-null   float64\n",
      " 27  E         2116 non-null   float64\n",
      " 28  AD        2116 non-null   float64\n",
      " 29  DE        2116 non-null   float64\n",
      " 30  LD        2116 non-null   float64\n",
      " 31  FS        2116 non-null   float64\n",
      " 32  SUSP      2116 non-null   float64\n",
      " 33  CLASS     2116 non-null   float64\n",
      " 34  NSP       2116 non-null   float64\n",
      "dtypes: float64(35)\n",
      "memory usage: 595.1 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# LOAD DATA\n",
    "df = pd.read_csv('../data/processed/cleaned_data.csv')\n",
    "\n",
    "# HANDLE DUPLICATES\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# HANDLE SKEWED FEATURES\n",
    "# Find skewed features, excluding the target 'NSP'\n",
    "numeric_features = df.select_dtypes(include=np.number)\n",
    "skewed_features = numeric_features.skew()\n",
    "skewed_features = skewed_features[abs(skewed_features) > 1]\n",
    "skewed_features = skewed_features.drop('NSP') # Exclude the target variable\n",
    "\n",
    "# Apply Yeo-Johnson transformation\n",
    "power_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "df[skewed_features.index] = power_transformer.fit_transform(df[skewed_features.index])\n",
    "\n",
    "# Drop any rows that might have NaN values after transformation\n",
    "df = df.dropna()\n",
    "\n",
    "# verify \n",
    "print(\"Data is now fully cleaned and preprocessed\")\n",
    "print(f\"Remaining number of rows: {len(df)}\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7927e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing SELECTIVE outlier capping...\n",
      "Selective outlier capping complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing SELECTIVE outlier capping...\")\n",
    "\n",
    "# create list of columns where outliers might be critical signals\n",
    "# dont cap these\n",
    "critical_signal_cols = ['AC', 'FM', 'UC', 'DL', 'DS', 'DP'] \n",
    "\n",
    "# get all numeric columns\n",
    "numeric_cols = df.drop(columns=['NSP']).select_dtypes(include=np.number).columns\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col not in critical_signal_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # cap the outliers\n",
    "        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n",
    "        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
    "\n",
    "print(\"Selective outlier capping complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b59f5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating smarter features based on medical context\n",
      "New features created.\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating smarter features based on medical context\")\n",
    "\n",
    "# Feature 1: Deceleration Severity\n",
    "# Combines the impact of long decelerations with abnormal heart rate variability.\n",
    "# A high value here is a potential red flag.\n",
    "df['Deceleration_Severity'] = df['DP'] * df['ASTV']\n",
    "\n",
    "# Feature 2: FHR Range\n",
    "# The difference between the max and min heart rate in the trace.\n",
    "# A very flat or very erratic range could be significant.\n",
    "df['FHR_Range'] = df['Max'] - df['Min']\n",
    "\n",
    "print(\"New features created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40d443d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping leaky 'CLASS' feature and separating X and y\n",
      "Data splitting complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "\n",
    "print(\"Dropping leaky 'CLASS' feature and separating X and y\")\n",
    "X = df.drop(columns=['NSP', 'CLASS'])\n",
    "y = df['NSP']\n",
    "\n",
    "# split the data into training and testing sets\n",
    "# use stratify=y to ensure both train and test sets have a similar class distribution.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(\"Data splitting complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac1cd598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test set at 20% of data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ba8eb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing feature selection...\n",
      "Original number of features: 35\n",
      "Selected number of features: 21\n",
      "Feature selection complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "print(\"Performing feature selection...\")\n",
    "\n",
    "# Use a Logistic Regression model with an 'l1' penalty to select the most important features.\n",
    "# This is a key step in preventing overfitting.\n",
    "selector = SelectFromModel(estimator=LogisticRegression(solver='liblinear', penalty='l1', C=0.1)).fit(X_train_scaled, y_train)\n",
    "\n",
    "# Create new datasets containing only the selected features\n",
    "X_train_selected = selector.transform(X_train_scaled)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Original number of features: {X_train.shape[1]}\")\n",
    "print(f\"Selected number of features: {X_train_selected.shape[1]}\")\n",
    "print(\"Feature selection complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Model Performance ---\n",
      "Balanced Accuracy: 0.7792595441851922\n",
      "Macro F1-Score: 0.7920803782505911\n"
     ]
    }
   ],
   "source": [
    "# TRAIN FINAL MODEL WITH REGULARIZATION (to prevent overfitting)\n",
    "# We use a smaller C value (e.g., C=0.1) for stronger regularization.\n",
    "final_model = LogisticRegression(C=0.1, max_iter=1000)\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# final model on the selected test data\n",
    "predictions = final_model.predict(X_test_selected)\n",
    "\n",
    "# Calculate the final scores as required by the datathon\n",
    "bal_accuracy = balanced_accuracy_score(y_test, predictions)\n",
    "macro_f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "print(f\"\\n--- Final Model Performance ---\")\n",
    "print(f\"Balanced Accuracy: {bal_accuracy}\")\n",
    "print(f\"Macro F1-Score: {macro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d9f7fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final clean data successfully saved to: ../data/processed/final_processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = '../data/processed/final_processed_data.csv'\n",
    "\n",
    "\n",
    "#use index=False to prevent pandas from writing the row numbers as a new column\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Final clean data successfully saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
